{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "# notebook_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "# os.chdir(notebook_dir)\n",
    "# %pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from src.spatial_prrp import run_prrp, run_parallel_prrp, grow_region\n",
    "from src.metis_parser import load_graph_from_metis\n",
    "from src.prrp_data_loader import load_shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Success Probablities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the list of following parameters for multiple runs\n",
    "parameters = {\n",
    "    \"num_regions\": [5, 10, 15],\n",
    "    \"cardinalities\": [10, 20, 30],\n",
    "    \"solutions_count\": [10, 20, 30],\n",
    "}\n",
    "\n",
    "success_probabilities = []\n",
    "def evaluate_success_probabilities(algorithm, area_data):\n",
    "    for num_regions in parameters[\"num_regions\"]:\n",
    "        for cardinality in parameters[\"cardinalities\"]:\n",
    "            for solutions_count in parameters[\"solutions_count\"]:\n",
    "                _, valid_solutions = algorithm(area_data, num_regions, cardinality, solutions_count)\n",
    "                success_prob = len(valid_solutions) / solutions_count\n",
    "                print(f\"Success probability for {num_regions} regions, {cardinality} cardinality, {solutions_count} solutions: {success_prob}\")\n",
    "                return success_prob\n",
    "            \n",
    "# Define the algorithms\n",
    "algorithms = {\n",
    "    \"PRRP\": run_parallel_prrp,  # Replace with actual function\n",
    "    \"PRRP-sequential\": run_prrp,  # Replace with actual function\n",
    "    \"PRRP-region-growth-only\": grow_region,  # Replace with actual function\n",
    "}\n",
    "\n",
    "# Path to the data folder\n",
    "data_folder = \"data\"\n",
    "shapefile_path = os.path.abspath(os.path.join(os.getcwd(), 'data/cb_2015_42_tract_500k/cb_2015_42_tract_500k.shp')) # get the absolute path to the shapefile.\n",
    "print(f\"Path to shape file : {shapefile_path}\")\n",
    "\n",
    "graphfile_path = os.path.abspath(os.path.join(os.getcwd(), 'data/PGPgiantcompo.graph')) # get the absolute path to the graphfile.\n",
    "print(f\"Path to graph file : {graphfile_path}\")\n",
    "\n",
    "# List of datasets\n",
    "datasets = [shapefile_path, graphfile_path]\n",
    "\n",
    "# Dictionary to store success probabilities\n",
    "success_probabilities = {alg: [] for alg in algorithms}\n",
    "\n",
    "# Evaluate each algorithm on each dataset\n",
    "for dataset_path in datasets:\n",
    "    if dataset_path.endswith('.graph'):\n",
    "        area_data = load_graph_from_metis(dataset_path)\n",
    "    else:\n",
    "        area_data = load_shapefile(dataset_path)\n",
    "    for alg_name, alg_func in algorithms.items():\n",
    "        success_prob = evaluate_algorithm(alg_func, area_data)\n",
    "        success_probabilities[alg_name].append(success_prob)\n",
    "\n",
    "# Calculate average success probabilities\n",
    "average_success_probabilities = {alg: np.mean(probs) for alg, probs in success_probabilities.items()}\n",
    "\n",
    "print(\"Average Success Probabilities:\")\n",
    "for alg, avg_prob in average_success_probabilities.items():\n",
    "    print(f\"{alg}: {avg_prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Effectiveness metric\n",
    "def evaluate_effectiveness(algorithm, area_data, parameters):\n",
    "    for num_regions in parameters[\"num_regions\"]:\n",
    "        for cardinality in parameters[\"cardinalities\"]:\n",
    "            for solutions_count in parameters[\"solutions_count\"]:\n",
    "                no_of_iterations, valid_solutions = algorithm(area_data, num_regions, cardinality, solutions_count)\n",
    "                effectiveness = len(valid_solutions) / no_of_iterations if no_of_iterations > 0 else 0\n",
    "                print(f\"Effectiveness for {num_regions} regions, {cardinality} cardinality, {solutions_count} solutions: {effectiveness}\")\n",
    "                return effectiveness\n",
    "            \n",
    "effectiveness = {alg: [] for alg in algorithms}\n",
    "\n",
    "# Evaluate each algorithm on each dataset\n",
    "for dataset_path in datasets:\n",
    "    if dataset_path.endswith('.graph'):\n",
    "        area_data = load_graph_from_metis(dataset_path)\n",
    "    else:\n",
    "        area_data = load_shapefile(dataset_path)\n",
    "    for alg_name, alg_func in algorithms.items():\n",
    "        effectiveness = evaluate_effectiveness(alg_func, area_data, parameters)\n",
    "        effectiveness[alg_name].append(effectiveness)\n",
    "\n",
    "# Calculate average effectiveness\n",
    "average_effectiveness = {alg: np.mean(eff) for alg, eff in effectiveness.items()}\n",
    "\n",
    "print(\"Average Effectiveness:\")\n",
    "for alg, avg_eff in average_effectiveness.items():\n",
    "    print(f\"{alg}: {avg_eff:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the execution time for the algorithms\n",
    "def evaluate_execution_time(algorithm, area_data, parameters):\n",
    "    for num_regions in parameters[\"num_regions\"]:\n",
    "        for cardinality in parameters[\"cardinalities\"]:\n",
    "            for solutions_count in parameters[\"solutions_count\"]:\n",
    "                # time before the algorithm starts\n",
    "                start_time = time.time()\n",
    "                _ = algorithm(area_data, num_regions, cardinality, solutions_count)\n",
    "                # time after the algorithm ends\n",
    "                end_time = time.time()\n",
    "                print(f\"Execution time for {num_regions} regions, {cardinality} cardinality, {solutions_count} solutions: {time_taken}\")\n",
    "                time_taken = end_time - start_time\n",
    "                return time_taken\n",
    "\n",
    "execution_times = {alg: [] for alg in algorithms}\n",
    "\n",
    "# Evaluate each algorithm on each dataset\n",
    "for dataset_path in datasets:\n",
    "    if dataset_path.endswith('.graph'):\n",
    "        area_data = load_graph_from_metis(dataset_path)\n",
    "    else:\n",
    "        area_data = load_shapefile(dataset_path)\n",
    "    for alg_name, alg_func in algorithms.items():\n",
    "        time_taken = evaluate_execution_time(alg_func, area_data, parameters)\n",
    "        execution_times[alg_name].append(time_taken)\n",
    "\n",
    "# Calculate average execution times\n",
    "average_execution_times = {alg: np.mean(times) for alg, times in execution_times.items()}\n",
    "print(\"Average Execution Times:\")\n",
    "\n",
    "for alg, avg_time in average_execution_times.items():\n",
    "    print(f\"{alg}: {avg_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the completeness metric given by the ratio of the number of regions in the solution to the number of regions in the optimal solution\n",
    "\n",
    "completeness_values = {alg: [] for alg in algorithms}\n",
    "\n",
    "def evaluate_completeness(algorithm, area_data, parameters):\n",
    "    for num_regions in parameters[\"num_regions\"]:\n",
    "        for cardinality in parameters[\"cardinalities\"]:\n",
    "            for solutions_count in parameters[\"solutions_count\"]:\n",
    "                _, valid_solutions = algorithm(area_data, num_regions, cardinality, solutions_count)\n",
    "                # Number of regions in the optimal solution\n",
    "                num_regions = len(area_data)\n",
    "                completeness = 0\n",
    "                for solution in valid_solutions:\n",
    "                    # Number of regions in the solution which is returned by the algorithm\n",
    "                    num_regions_in_solution = len(solution)\n",
    "                    # Completeness metric\n",
    "                    completeness = completeness + num_regions_in_solution / num_regions\n",
    "                print(f\"Completeness for {num_regions} regions, {cardinality} cardinality, {solutions_count} solutions: {completeness}\")\n",
    "                completeness_values.append(completeness)\n",
    "    return completeness_values\n",
    "\n",
    "# Evaluate each algorithm on each dataset\n",
    "for dataset_path in datasets:\n",
    "    if dataset_path.endswith('.graph'):\n",
    "        area_data = load_graph_from_metis(dataset_path)\n",
    "    else:\n",
    "        area_data = load_shapefile(dataset_path)\n",
    "    for alg_name, alg_func in algorithms.items():\n",
    "        completeness = evaluate_completeness(alg_func, area_data, parameters)\n",
    "        completeness_values[alg_name].append(completeness)\n",
    "\n",
    "# Calculate average completeness values\n",
    "average_completeness_values = {alg: np.mean(completeness) for alg, completeness in completeness_values.items()}\n",
    "print(\"Average Completeness Values:\")\n",
    "\n",
    "for alg, avg_completeness in average_completeness_values.items():\n",
    "    print(f\"{alg}: {avg_completeness:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
