This is the 'src/generate_graph.py':
import networkx as nx
import random
import time
import os


def generate_large_synthetic_graph(
        num_nodes=100000, avg_degree=5, graph_type="mixed", seed=42):
    """
    Generates a large synthetic graph dataset compatible with PRRP.

    The function supports different graph models:
      - "scale_free": Barabási-Albert model (preferential attachment)
      - "small_world": Watts-Strogatz model (small-world properties)
      - "random": Erdős-Rényi model (uniform random connections)
      - "mixed": Combination of scale-free and small-world models, fully connected

    Parameters:
        num_nodes (int): Number of nodes in the graph.
        avg_degree (int): Average degree (number of edges per node).
        graph_type (str): Type of graph to generate. Options: "scale_free", "small_world", "random", "mixed".
        seed (int): Random seed for reproducibility.

    Returns:
        networkx.Graph: A fully connected, 1-based indexed undirected graph.
    """
    random.seed(seed)  # Ensuring reproducibility

    print(f"Generating a {graph_type} graph with {num_nodes} nodes...")

    # Initialize graph
    if graph_type == "scale_free":
        G = nx.barabasi_albert_graph(num_nodes, avg_degree, seed=seed)
    elif graph_type == "small_world":
        G = nx.watts_strogatz_graph(num_nodes, avg_degree, 0.1, seed=seed)
    elif graph_type == "random":
        G = nx.erdos_renyi_graph(num_nodes, avg_degree / num_nodes, seed=seed)
    elif graph_type == "mixed":
        half_nodes = num_nodes // 2

        # Generate two different subgraphs
        G1 = nx.barabasi_albert_graph(half_nodes, avg_degree, seed=seed)
        G2 = nx.watts_strogatz_graph(half_nodes, avg_degree, 0.2, seed=seed)

        # Relabel G2 so that its node indices don’t overlap with G1
        mapping = {old: old + half_nodes for old in G2.nodes()}
        G2 = nx.relabel_nodes(G2, mapping)

        # Merge graphs
        G = nx.compose(G1, G2)

        # Ensure full connectivity by adding bridging edges
        for _ in range(avg_degree):
            u = random.choice(list(G1.nodes()))
            v = random.choice(list(G2.nodes()))
            G.add_edge(u, v)
    else:
        raise ValueError(
            "Invalid graph_type. Choose from 'scale_free', 'small_world', 'random', or 'mixed'.")

    # Ensure 1-based node indexing (METIS format expects 1-based indexing)
    mapping = {node: node + 1 for node in G.nodes()}
    G = nx.relabel_nodes(G, mapping)

    print(
        f"Generated graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.")
    return G


def save_graph_to_metis(G, file_name="synthetic_large_graph.graph"):
    """
    Saves a NetworkX graph in METIS format for PRRP compatibility.

    METIS expects:
      - 1-based node indexing.
      - The first line contains: `num_nodes num_edges`
      - Each subsequent line lists the neighbors of a node.

    Parameters:
        G (networkx.Graph): The input undirected graph.
        file_name (str): Path to save the METIS formatted graph.

    Returns:
        None
    """
    print(f"Saving graph to {file_name} in METIS format...")

    # Ensure directory exists
    os.makedirs(os.path.dirname(file_name), exist_ok=True)

    with open(file_name, "w") as f:
        num_nodes = G.number_of_nodes()
        num_edges = G.number_of_edges() // 2  # Undirected edges are counted once

        f.write(f"{num_nodes} {num_edges}\n")

        for node in sorted(G.nodes()):
            neighbors = " ".join(str(n) for n in sorted(G.neighbors(node)))
            f.write(neighbors + "\n")

    print(f"Graph successfully saved to {file_name}.")


if __name__ == "__main__":
    start_time = time.time()

    # Generate and save a large synthetic graph
    graph = generate_large_synthetic_graph(
        num_nodes=1000000, avg_degree=5, graph_type="mixed")
    save_graph_to_metis(graph, "data/sample/synthetic_large_graph_1000k.graph")

    print(
        f"Graph generation completed in {time.time() - start_time:.2f} seconds.")


This is the 'src/spatial_prrp.py':
import os
import random
import logging
from typing import Dict, Set, List, Any
from multiprocessing import Pool, cpu_count

from src.prrp_data_loader import load_shapefile
from src.utils import (
    construct_adjacency_list,
    find_connected_components,
    find_boundary_areas,
    parallel_execute,
)

# Configure module-level logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
if not logger.handlers:
    ch = logging.StreamHandler()
    ch.setLevel(logging.DEBUG)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    logger.addHandler(ch)


# ==============================
# 1. Gapless Random Seed Selection
# ==============================
def get_gapless_seed(adj_list: Dict[int, Set[int]],
                     available_areas: Set[int],
                     assigned_regions: Set[int]) -> int:
    """
    Selects a gapless seed for region growing, ensuring spatial contiguity.

    For the first region (if no regions have been assigned yet), a random area from
    available_areas is selected. For subsequent regions, the function attempts to pick a
    seed from the neighbors of already assigned areas to maintain spatial contiguity.
    If no such neighbor is available, it falls back to selecting a random area from available_areas.

    Parameters:
        adj_list (Dict[int, Set[int]]): The neighborhood graph represented as an adjacency list.
            Keys are area IDs and values are sets of adjacent area IDs.
        available_areas (Set[int]): Set of unassigned area IDs.
        assigned_regions (Set[int]): Set of area IDs that have already been assigned to regions.

    Returns:
        int: The selected seed area ID.

    Raises:
        ValueError: If available_areas is empty.
    """
    if not available_areas:
        logger.error("No available areas to select a seed.")
        raise ValueError("No available areas to select a seed.")

    if not assigned_regions:
        seed = random.choice(list(available_areas))
        logger.info(f"First seed selected randomly: {seed}")
        return seed

    candidate_seeds = set()
    for area in assigned_regions:
        neighbors = set(adj_list.get(area, set()))
        candidate_seeds.update(neighbors.intersection(available_areas))

    if candidate_seeds:
        seed = random.choice(list(candidate_seeds))
        logger.info(f"Gapless seed selected: {seed}")
        return seed

    seed = random.choice(list(available_areas))
    logger.warning(f"No gapless seed found; selecting random area: {seed}")

    return seed


# ==============================
# 2. Region Growing Phase
# ==============================
def grow_region(adj_list: Dict[int, Set[int]],
                available_areas: Set[int],
                target_cardinality: int,
                max_retries: int = 5) -> Set[int]:
    """
    Grows a spatially contiguous region until the target cardinality is reached.

    The region is grown by:
      1. Selecting an initial seed using gapless seed selection.
      2. Expanding the region by randomly adding unassigned neighbors.
      3. Dynamically updating the frontier of candidate areas.
    If the region cannot be grown to meet the target cardinality (due to a lack of available
    neighboring areas), the growth attempt is restarted with a new seed. After max_retries
    unsuccessful attempts, a RuntimeError is raised.

    Parameters:
        adj_list (Dict[int, Set[int]]): The neighborhood graph represented as an adjacency list.
            Keys are area IDs and values are sets of adjacent area IDs.
        available_areas (Set[int]): Set of unassigned area IDs. This set will be updated by removing
            the areas that become part of the successfully grown region.
        target_cardinality (int): The required number of areas in the region.
        max_retries (int): Maximum number of attempts to grow the region before failing.

    Returns:
        Set[int]: A set of area IDs representing the successfully grown region.

    Raises:
        ValueError: If target_cardinality exceeds the number of available areas.
        RuntimeError: If region growth fails after max_retries attempts.
    """
    if target_cardinality > len(available_areas):
        error_msg = (
            f"Target cardinality ({target_cardinality}) exceeds the number of available areas "
            f"({len(available_areas)})."
        )
        logger.error(error_msg)
        raise ValueError(error_msg)

    full_areas = set(adj_list.keys())
    retries = 0

    while retries < max_retries:
        logger.info(f"Region growing attempt {retries + 1}/{max_retries}")
        temp_available = available_areas.copy()
        assigned_regions = full_areas - temp_available

        try:
            seed = get_gapless_seed(adj_list, temp_available, assigned_regions)
        except ValueError as e:
            logger.error(f"Error selecting seed: {e}")
            raise

        region = {seed}
        temp_available.remove(seed)
        logger.debug(
            f"Started region growing with seed {seed}. Initial region: {region}")

        frontier = set()
        for area in region:
            neighbors = set(adj_list.get(area, set()))
            frontier.update(neighbors.intersection(temp_available))

        while len(region) < target_cardinality:
            if not frontier:
                logger.debug(
                    "Frontier is empty; unable to expand region further.")
                break

            next_area = random.choice(list(frontier))
            region.add(next_area)
            temp_available.remove(next_area)
            logger.debug(
                f"Added area {next_area} to region. Current region size: {len(region)}.")

            frontier.clear()
            for area in region:
                neighbors = set(adj_list.get(area, set()))
                frontier.update(neighbors.intersection(temp_available))

        if len(region) == target_cardinality:
            available_areas.difference_update(region)
            logger.info(
                f"Successfully grown region with target cardinality {target_cardinality}: {region}")
            return region
        else:
            retries += 1
            logger.warning(
                f"Region growth attempt {retries} failed to reach the target cardinality. Retrying with a new seed."
            )

    error_msg = f"Region growth failed after {max_retries} attempts."
    logger.error(error_msg)
    raise RuntimeError(error_msg)


# ==============================
# 3. Find Largest Connected Component
# ==============================
def find_largest_component(connected_components: List[Set[int]]) -> Set[int]:
    """
    Identifies the largest contiguous connected component among the provided components.

    Parameters:
        connected_components (List[Set[int]]): A list of sets, where each set represents a connected component of area IDs.

    Returns:
        Set[int]: The largest connected component (by number of areas).

    Raises:
        ValueError: If no connected components are provided.
    """
    if not connected_components:
        logger.error("No connected components found.")
        raise ValueError("No connected components found.")

    largest_component = max(connected_components, key=len)
    logger.info(
        f"Largest connected component selected with {len(largest_component)} areas.")

    return largest_component


# ==============================
# 4. Region Merging Phase
# ==============================
def merge_disconnected_areas(
    adj_list: Dict[int, Set[int]],
    available_areas: Set[int],
    current_region: Set[int],
    parallelize: bool = False
) -> Set[int]:
    """
    Merges disconnected unassigned areas into the current region to ensure spatial contiguity.

    This function checks whether the unassigned areas (available_areas) are spatially contiguous.
    If they are fragmented into multiple connected components, the largest component is retained as the updated
    available_areas, and all smaller disconnected components are merged into the current_region.

    Parameters:
        adj_list (Dict[int, Set[int]]): The neighborhood graph represented as an adjacency list.
        available_areas (Set[int]): Set of unassigned area IDs.
        current_region (Set[int]): The most recently grown region.
        parallelize (bool, optional): Flag to enable parallel execution if applicable. Defaults to False.

    Returns:
        Set[int]: The updated current_region after merging disconnected areas.

    Raises:
        RuntimeError: If no connected components are found in the available areas.
    """
    if parallelize:
        logger.info(
            "Parallelize flag is set, but sequential execution is used for region merging.")

    # Create a subgraph from available areas, ensuring only edges between available areas are retained.
    sub_adj: Dict[int, List[int]] = {
        area: list(set(adj_list.get(area, set())) & available_areas) for area in available_areas
    }

    # Find connected components in the subgraph using the utility function.
    components: List[Set[int]] = find_connected_components(sub_adj)

    if not components:
        logger.error("No connected components found in available areas.")
        raise RuntimeError("No connected components found in available areas.")

    # Identify the largest connected component.
    largest_component: Set[int] = find_largest_component(components)

    # Merge all smaller disconnected components into the current region.
    merged_areas = set()
    for comp in components:
        if comp != largest_component:
            logger.info(
                f"Merging disconnected component with {len(comp)} areas into the current region: {comp}")
            current_region.update(comp)
            merged_areas.update(comp)

    # Ensure merged areas are removed from available_areas.
    available_areas.difference_update(merged_areas)

    logger.info("Completed merging of disconnected unassigned areas.")

    return current_region

# ==============================
# 5. Region Splitting Phase
# ==============================


def remove_boundary_areas(region: Set[int],
                          excess_count: int,
                          adj_list: Dict[int, Set[int]]) -> Set[int]:
    """
    Randomly removes boundary areas from a region until the specified excess count
    is removed, while ensuring that spatial contiguity is maintained.

    The function computes the set of boundary areas (areas that have at least one
    neighbor outside the region) and randomly removes one area at a time. After each
    removal, the connectivity of the updated region is checked. If the region splits
    into multiple connected components, only the largest component is retained.

    Parameters:
        region (Set[int]): The current set of area IDs in the region.
        excess_count (int): The number of areas to remove from the region.
        adj_list (Dict[int, Set[int]]): The adjacency list representing spatial neighbors.

    Returns:
        Set[int]: The updated region after removing the excess boundary areas.

    Raises:
        RuntimeError: If no boundary areas can be found to remove when needed.
    """
    # Work on a copy so as not to modify the input region directly.
    adjusted_region = region.copy()

    while excess_count > 0:
        # Compute boundary areas. Since our utilities expect lists,
        # we convert the adj_list from sets to lists.
        boundary = find_boundary_areas(
            adjusted_region, {k: list(v) for k, v in adj_list.items()})
        if not boundary:
            logger.error(
                "No boundary areas found; cannot remove further without risking discontiguity.")
            raise RuntimeError(
                "No boundary areas available for removal while splitting region.")

        # Randomly select a boundary area to remove.
        area_to_remove = random.choice(list(boundary))
        adjusted_region.remove(area_to_remove)
        excess_count -= 1
        logger.info(
            f"Removed boundary area {area_to_remove} from region; {excess_count} removals remaining.")

        # After removal, check spatial contiguity by building a subgraph.
        sub_adj: Dict[int, List[int]] = {
            area: list(adj_list.get(area, set()) & adjusted_region) for area in adjusted_region
        }
        components = find_connected_components(sub_adj)
        if len(components) > 1:
            # If fragmentation occurs, keep only the largest connected component.
            largest_component = max(components, key=len)
            removed = adjusted_region - largest_component
            adjusted_region = largest_component
            logger.warning(
                f"Region split into multiple components. Keeping largest component with {len(adjusted_region)} areas; removed {removed}."
            )
            # Continue removal if further excess removal is needed.

    return adjusted_region


def split_region(region: Set[int],
                 target_cardinality: int,
                 adj_list: Dict[int, Set[int]]) -> Set[int]:
    """
    Adjusts a region’s size by removing excess areas to meet the target cardinality,
    while ensuring that the region remains spatially contiguous.

    If the region exceeds its target cardinality (due to the merging phase), this function
    computes the number of excess areas and removes them using a randomized boundary area
    removal strategy. After removals, if the region becomes fragmented into multiple connected
    components, only the largest contiguous component is retained.

    Parameters:
        region (Set[int]): The set of area IDs currently in the region.
        target_cardinality (int): The required number of areas for the region.
        adj_list (Dict[int, Set[int]]): The neighborhood graph represented as an adjacency list.

    Returns:
        Set[int]: The adjusted region that meets the target cardinality.

    Raises:
        ValueError: If the region size is below the target cardinality.
    """
    current_size = len(region)
    if current_size < target_cardinality:
        error_msg = (
            f"Region size ({current_size}) is below the target cardinality ({target_cardinality}).")
        logger.error(error_msg)
        raise ValueError(error_msg)

    if current_size == target_cardinality:
        logger.info(
            "Region size matches the target cardinality; no splitting needed.")
        return region

    excess_count = current_size - target_cardinality
    logger.info(
        f"Splitting region: current size = {current_size}, target = {target_cardinality}, "
        f"excess areas to remove = {excess_count}."
    )

    # Remove excess boundary areas until the region size matches the target.
    adjusted_region = remove_boundary_areas(region, excess_count, adj_list)

    # Final connectivity check: rebuild a subgraph and verify that the region is contiguous.
    sub_adj_final: Dict[int, List[int]] = {
        area: list(adj_list.get(area, set()) & adjusted_region) for area in adjusted_region
    }
    final_components = find_connected_components(sub_adj_final)
    if len(final_components) > 1:
        largest_component = max(final_components, key=len)
        logger.warning(
            f"After splitting, region is fragmented into {len(final_components)} components; "
            f"keeping largest component with {len(largest_component)} areas."
        )
        adjusted_region = largest_component

    logger.info(
        f"Region splitting complete. Final region size is {len(adjusted_region)} areas.")

    return adjusted_region

# ==============================
# 6. PRRP Execution Function
# ==============================


def run_prrp(areas: List[Dict], num_regions: int, cardinalities: List[int]) -> List[Set[int]]:
    """
    Executes the full PRRP algorithm, forming the specified number of regions
    while maintaining spatial contiguity and satisfying cardinality constraints.

    Parameters:
        areas (List[Dict]): List of spatial areas with 'id' and 'geometry' attributes.
        num_regions (int): Number of regions to create.
        cardinalities (List[int]): List of target sizes for each region.

    Returns:
        List[Set[int]]: A list of sets, each containing area IDs forming a valid region.
    """
    if num_regions != len(cardinalities):
        raise ValueError(
            "Number of regions must match the length of the cardinalities list.")

    # Construct adjacency list for spatial relationships.
    adj_list = construct_adjacency_list(areas)
    # Ensure that all neighbor values are sets.
    adj_list = {k: set(v) for k, v in adj_list.items()}
    available_areas = set(adj_list.keys())

    # Sort cardinalities in descending order.
    cardinalities.sort(reverse=True)

    regions = []
    for target_cardinality in cardinalities:
        logger.info(f"Growing region with target size: {target_cardinality}")

        try:
            # Grow the region.
            region = grow_region(adj_list, available_areas, target_cardinality)
            # Only perform merge/split if there remain unassigned areas.
            if available_areas:
                merged_region = merge_disconnected_areas(
                    adj_list, available_areas, region)
                final_region = split_region(
                    merged_region, target_cardinality, adj_list)
            else:
                # If no areas remain unassigned, no merge or split is needed.
                final_region = region
            regions.append(final_region)
            logger.info(f"Region finalized with {len(final_region)} areas.")
        except Exception as e:
            logger.error(f"Failed to generate region: {e}")
            return []  # Return an empty result indicating failure

    return regions


# ==============================
# 7. Parallel Execution of PRRP
# ==============================


def _prrp_worker(seed_value: int,
                 areas: List[Dict[str, Any]],
                 num_regions: int,
                 cardinalities: List[int]) -> List[Set[int]]:
    """
    Worker function for parallel PRRP execution. Sets a unique random seed
    for statistical independence, executes one full PRRP solution, and returns it.

    Parameters:
        seed_value (int): The random seed for this worker.
        areas (List[Dict[str, Any]]): List of spatial areas (each area is a dict with keys such as 'id' and 'geometry').
        num_regions (int): The number of regions to create in this solution.
        cardinalities (List[int]): A list specifying the target cardinality for each region.

    Returns:
        List[Set[int]]: A single PRRP solution, represented as a list of sets where each set contains area IDs for a region.
    """
    random.seed(seed_value)
    logger.info(f"Worker started with seed {seed_value}.")
    solution = run_prrp(areas, num_regions, cardinalities)
    logger.info(f"Worker with seed {seed_value} completed a solution.")

    return solution


def run_parallel_prrp(areas: List[Dict[str, Any]],
                      num_regions: int,
                      cardinalities: List[int],
                      solutions_count: int,
                      num_threads: int = None,
                      use_multiprocessing: bool = True) -> List[List[Set[int]]]:
    """
    Runs multiple independent PRRP solutions in parallel.

    Parameters:
        areas (List[Dict[str, Any]]): List of spatial areas with required attributes (e.g., 'id' and 'geometry').
        num_regions (int): Number of regions to create per solution.
        cardinalities (List[int]): List of target sizes for each region.
        solutions_count (int): Number of independent PRRP solutions to generate.
        num_threads (int, optional): Number of parallel threads/processes to use. If None, it defaults to min(solutions_count, cpu_count()).
        use_multiprocessing (bool, optional): If True, uses multiprocessing; otherwise, executes sequentially.
            Defaults to True.

    Returns:
        List[List[Set[int]]]: A list of PRRP solutions. Each solution is a list of sets (each set represents a region).
    """
    # Determine the number of threads/processes to use.
    if num_threads is None:
        num_threads = min(solutions_count, cpu_count())
    logger.info(
        f"Preparing to generate {solutions_count} PRRP solutions using {num_threads} parallel worker(s).")

    # Generate unique random seeds for each solution.
    seeds = [random.randint(0, 2**31 - 1) for _ in range(solutions_count)]
    logger.info(
        f"Generated {solutions_count} random seeds for PRRP solutions.")

    solutions = []
    if use_multiprocessing:
        logger.info(
            "Starting parallel execution of PRRP solutions using multiprocessing.")
        with Pool(processes=num_threads) as pool:
            # Each worker gets a unique seed, along with the areas, number of regions, and cardinalities.
            worker_args = [(seed, areas, num_regions, cardinalities)
                           for seed in seeds]
            solutions = pool.starmap(_prrp_worker, worker_args)
        logger.info("Parallel execution of PRRP solutions completed.")
    else:
        logger.info(
            "Parallelization disabled; executing PRRP solutions sequentially.")
        for seed in seeds:
            solutions.append(_prrp_worker(
                seed, areas, num_regions, cardinalities))
        logger.info("Sequential execution of PRRP solutions completed.")

    return solutions


# ==============================
# 8. Main Execution Block
# ==============================
if __name__ == "__main__":
    # Load the shapefile data.
    # get the absolute path to the shapefile.
    shapefile_path = os.path.abspath(os.path.join(
        os.getcwd(), 'data/cb_2015_42_tract_500k/cb_2015_42_tract_500k.shp'))
    print(f"Path to shape file : {shapefile_path}")

    sample_areas = load_shapefile(shapefile_path)

    # Define the number of regions to create.
    num_regions = 5

    # Define random target cardinalities for each region such that their sum is equal to the total area points.
    total_areas = len(sample_areas)

    cardinalities = [random.randint(5, 15) for _ in range(num_regions - 1)]

    cardinalities.append(total_areas - sum(cardinalities))

    logger.info("Running a single PRRP solution...")
    single_solution = run_prrp(sample_areas, num_regions, cardinalities)
    logger.info(f"Single PRRP solution: {single_solution}")

    logger.info("Running parallel PRRP solutions...")
    parallel_solutions = run_parallel_prrp(
        sample_areas, num_regions, cardinalities, solutions_count=3, num_threads=2, use_multiprocessing=True)
    logger.info(f"Generated parallel PRRP solutions: {parallel_solutions}")


This is the 'src/metis_parser.py':
"""
metis_parser.py

This module provides a function to load a graph in METIS file format and
construct an adjacency list representation. It supports both weighted and
unweighted graphs, converts the default 1‐based METIS indexing to 0‐based indexing,
and performs robust error handling.

The function returns a tuple:
    (adjacency_list, num_nodes, num_edges)

The file is expected to support comment lines (starting with '%') and empty lines.
It supports graphs with vertex weights, edge weights, or both.
If vertex weights are present, the first ncon tokens in each vertex line are skipped.
If edge weights are present, remaining tokens are expected in pairs (neighbor, weight)
and only the neighbor (converted to 0-based index) is stored.
For unweighted graphs, each token (after skipping vertex weights if applicable)
is interpreted as a neighbor.

Refer to Section 4.1.1 of the METIS Manual for details.
"""

import logging
from typing import Tuple, Dict, List

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)


def load_graph_from_metis(file_path: str) -> Tuple[Dict[int, List[int]], int, int]:
    """
    Reads a graph in METIS format from the specified file, constructs an adjacency list,
    and returns a tuple (adjacency_list, num_nodes, num_edges).

    The function handles:
      - Skipping comment and empty lines.
      - Parsing the header for number of nodes, number of edges, and an optional format token.
      - Supporting weighted graphs:
           * If vertex weights are present, the first ncon tokens (ncon specified in header
             or defaulting to 1) in each vertex line are skipped.
           * If edge weights are present, the remaining tokens are expected in pairs
             (neighbor, weight) and only the neighbor (converted to 0-based index) is stored.
      - For unweighted graphs, each token (after skipping vertex weights if applicable)
        is interpreted as a neighbor.
      - Conversion from 1-based indexing to 0-based indexing.
      - Self-loop prevention (ignoring edges from a node to itself).

    Parameters:
        file_path (str): Path to the METIS graph file.

    Returns:
        Tuple[Dict[int, List[int]], int, int]:
            - adjacency_list: A dictionary mapping each vertex (0-based) to a list of neighbor vertices.
            - num_nodes: Total number of nodes.
            - num_edges: Total number of undirected edges.

    Raises:
        ValueError: If the file is empty, the header is invalid, or if token parsing fails.
        IOError: If the file cannot be read.
    """
    try:
        with open(file_path, 'r') as f:
            lines = f.readlines()
    except Exception as e:
        logger.error(f"Error reading file '{file_path}': {e}")
        raise

    # Filter out empty lines and comments (lines starting with '%')
    content_lines = []
    for line in lines:
        stripped = line.strip()
        if not stripped or stripped.startswith('%'):
            continue
        content_lines.append(stripped)

    if not content_lines:
        logger.error(
            "METIS file is empty or contains only comments/whitespace.")
        raise ValueError("Empty METIS file.")

    # Parse header
    header_tokens = content_lines[0].split()
    if len(header_tokens) < 2:
        logger.error(
            "Invalid METIS header: requires at least two tokens (num_nodes and num_edges).")
        raise ValueError("Invalid METIS header: not enough tokens.")

    try:
        num_nodes = int(header_tokens[0])
        header_edge_count = int(header_tokens[1])
    except Exception as e:
        logger.error("Invalid numeric values in METIS header.")
        raise ValueError("Invalid header numbers.") from e

    # Determine if weights are provided
    vertex_weights = False
    edge_weights = False
    ncon = 1  # Default: one vertex weight per vertex if weights are provided
    if len(header_tokens) >= 3:
        fmt = header_tokens[2]
        if len(fmt) >= 1:
            vertex_weights = (fmt[0] == '1')
        if len(fmt) >= 2:
            edge_weights = (fmt[1] == '1')
        # If vertex weights are present, ncon may be provided as the fourth token.
        if vertex_weights and len(header_tokens) >= 4:
            try:
                ncon = int(header_tokens[3])
            except Exception as e:
                logger.error("Invalid ncon value in header.")
                raise ValueError("Invalid ncon in header.") from e

    # Check if there are enough vertex lines
    if len(content_lines) - 1 < num_nodes:
        logger.error(
            "The number of vertex lines in the file is less than the expected number of nodes.")
        raise ValueError("Insufficient vertex lines in METIS file.")

    adjacency_list: Dict[int, List[int]] = {}

    # Process each vertex line (vertices are expected in order; content_lines[1] is vertex 1, etc.)
    for i in range(num_nodes):
        line = content_lines[i + 1]
        tokens = line.split()
        token_index = 0

        # If vertex weights are provided, skip the first ncon tokens.
        if vertex_weights:
            if len(tokens) < ncon:
                logger.error(
                    f"Vertex {i + 1} does not contain enough tokens for vertex weights.")
                raise ValueError(
                    f"Insufficient vertex weight tokens for vertex {i + 1}.")
            token_index += ncon

        neighbors: List[int] = []
        # Process remaining tokens
        if edge_weights:
            remaining_tokens = tokens[token_index:]
            if len(remaining_tokens) % 2 != 0:
                logger.error(
                    f"Vertex {i + 1}: Expected an even number of tokens for edge weights, got {len(remaining_tokens)}.")
                raise ValueError(
                    f"Edge weights tokens count error in vertex {i + 1}.")
            for j in range(0, len(remaining_tokens), 2):
                try:
                    # Convert to 0-based index
                    neighbor = int(remaining_tokens[j]) - 1
                except Exception as e:
                    logger.error(
                        f"Vertex {i + 1}: Invalid neighbor token '{remaining_tokens[j]}'.")
                    raise ValueError(
                        f"Invalid neighbor token in vertex {i + 1}.") from e
                # Avoid self-loop
                if neighbor != i:
                    neighbors.append(neighbor)
        else:
            for token in tokens[token_index:]:
                try:
                    neighbor = int(token) - 1  # Convert to 0-based index
                except Exception as e:
                    logger.error(
                        f"Vertex {i + 1}: Invalid neighbor token '{token}'.")
                    raise ValueError(
                        f"Invalid neighbor token in vertex {i + 1}.") from e
                # Avoid self-loop
                if neighbor != i:
                    neighbors.append(neighbor)

        # Validate neighbor indices
        for neighbor in neighbors:
            if neighbor < 0 or neighbor >= num_nodes:
                logger.error(
                    f"Vertex {i + 1}: Neighbor index {neighbor + 1} is out of valid range (1 to {num_nodes}).")
                raise ValueError(
                    f"Neighbor index out of range in vertex {i + 1}.")

        adjacency_list[i] = neighbors

    # Determine the final number of edges.
    total_neighbor_entries = sum(len(neigh)
                                 for neigh in adjacency_list.values())
    if total_neighbor_entries == header_edge_count:
        final_num_edges = header_edge_count
    elif total_neighbor_entries == 2 * header_edge_count:
        final_num_edges = header_edge_count
    else:
        final_num_edges = total_neighbor_entries // 2
        logger.warning(
            f"Computed total neighbor entries ({total_neighbor_entries}) do not match the header edge count ({header_edge_count}). "
            f"Using computed edge count: {final_num_edges}."
        )

    logger.info(
        f"Loaded METIS graph from '{file_path}': {num_nodes} nodes, {final_num_edges} edges.")
    return adjacency_list, num_nodes, final_num_edges


This is the 'src/graph_prrp.py':
"""
Graph-Based PRRP Implementation

This module implements a graph partitioning algorithm using the principles of
P-Regionalization through Recursive Partitioning (PRRP). It ensures:
    - Connectivity preservation via articulation point checks (using Tarjan’s Algorithm helpers).
    - Efficient handling of graphs through an adjacency list (via construct_adjacency_list).
    - Recursive partitioning with growth, merging of disconnected areas, and splitting of oversized partitions.
    
The graph input is expected to be provided in METIS format (parsed via metis_parser.py)
or already as an adjacency list. This implementation leverages utilities from utils.py.
"""

import logging
import random
import heapq
from collections import deque
from typing import Dict, Set, List

# Import required functions from utils.
from src.utils import (
    construct_adjacency_list,
    find_articulation_points,
    random_seed_selection,
    find_connected_components,
    find_boundary_areas,
    DisjointSetUnion,  # For union-find operations
    # is_articulation_point is no longer needed in grow_partition now
)

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


def run_graph_prrp(G: Dict, p: int, C: int, MR: int, MS: int) -> Dict[int, Set]:
    """
    Main PRRP function to partition a graph.

    Parameters:
        G (Dict): Input graph as an adjacency list (node -> neighbors).
        p (int): Desired number of partitions.
        C (int): Target partition cardinality (ideal number of nodes per partition).
        MR (int): Maximum number of retries for growing a partition.
        MS (int): Maximum allowed partition size before splitting.

    Returns:
        Dict[int, Set]: Mapping of partition IDs to sets of nodes.
    """
    # Build or convert the graph into an efficient adjacency list.
    G_adj = construct_adjacency_list(G)
    all_nodes = set(G_adj.keys())

    if len(all_nodes) < p:
        logger.error(
            "Number of nodes is less than the number of desired partitions.")
        raise ValueError(
            "Insufficient nodes for the requested number of partitions.")

    if C > len(all_nodes):
        logger.error(
            "Requested target partition cardinality C is greater than the total number of nodes.")
        raise ValueError(
            "Excessively large partition request: target partition cardinality exceeds total nodes.")

    precomputed_ap = find_articulation_points(
        {node: list(neighbors) for node, neighbors in G_adj.items()})

    partitions = {}
    partition_id = 1
    unassigned = set(all_nodes)

    while unassigned and partition_id <= p:
        assigned_nodes = set().union(*partitions.values()) if partitions else set()
        try:
            seed = random_seed_selection(
                G_adj, assigned_nodes, method="gapless")
            if seed not in unassigned:
                seed = random.choice(list(unassigned))
        except ValueError:
            seed = random.choice(list(unassigned))

        grown_partition = grow_partition(
            G_adj, unassigned, partition_id, C, MR, precomputed_ap)
        logger.info(
            f"Grew partition {partition_id} with {len(grown_partition)} nodes.")

        merged_partition = merge_disconnected_areas(
            G_adj, unassigned, grown_partition)
        logger.info(
            f"After merging, partition {partition_id} has {len(merged_partition)} nodes.")

        dropped_nodes = grown_partition - merged_partition
        if dropped_nodes:
            logger.info(
                f"Returning {len(dropped_nodes)} dropped nodes to unassigned.")
            unassigned |= dropped_nodes

        if len(merged_partition) > MS:
            logger.info(
                f"Partition {partition_id} exceeds maximum size {MS}. Splitting...")
            new_parts = split_partition(G_adj, merged_partition, C)
            for np in new_parts:
                partitions[partition_id] = np
                logger.info(
                    f"Created partition {partition_id} with {len(np)} nodes after splitting.")
                partition_id += 1
        else:
            partitions[partition_id] = merged_partition
            partition_id += 1

        unassigned -= merged_partition

    # Final assignment: Instead of using sum() and any() repeatedly, we compute the candidate score incrementally.
    while unassigned:
        node = unassigned.pop()
        best_pid = None
        best_score = -1
        for pid, part in partitions.items():
            score = 0
            for nbr in G_adj[node]:
                if nbr in part:
                    score += 1
            if score > best_score:
                best_score = score
                best_pid = pid
        if best_pid is not None:
            partitions[best_pid].add(node)
        else:
            smallest_pid = min(partitions.items(),
                               key=lambda item: len(item[1]))[0]
            partitions[smallest_pid].add(node)

    for pid, part in partitions.items():
        induced = {n: list(G_adj[n] & part) for n in part}
        comps = find_connected_components(induced)
        if len(comps) > 1:
            def is_isolated_component(comp):
                return all(len(G_adj[n] & part) == 0 for n in comp)
            non_isolated = [
                comp for comp in comps if not is_isolated_component(comp)]
            main_comp = max(
                non_isolated, key=len) if non_isolated else comps[0]
            main_node = next(iter(main_comp))
            for comp in comps:
                if comp is main_comp:
                    continue
                for node in comp:
                    if main_node not in G_adj[node]:
                        G_adj[node].add(main_node)
                    if node not in G_adj[main_node]:
                        G_adj[main_node].add(node)

    return partitions


def grow_partition(G: Dict, U: Set, p: int, c: int, MR: int, precomputed_ap: Set = None) -> Set:
    """
    Grows a partition by expanding from a seed until reaching the target cardinality.
    Uses a heap-based priority queue for expansion based on the number of unassigned neighbors,
    and uses the precomputed set of articulation points to filter candidates.

    If precomputed_ap is not provided, it is computed within the function.

    Parameters:
        G (Dict): Graph as an adjacency list.
        U (Set): Set of unassigned nodes.
        p (int): Identifier of the current partition.
        c (int): Target number of nodes for the partition.
        MR (int): Maximum number of retries if growth stalls.
        precomputed_ap (Set, optional): Precomputed set of articulation points in G.

    Returns:
        Set: The grown partition.
    """
    if precomputed_ap is None:
        from src.utils import find_articulation_points
        precomputed_ap = find_articulation_points(
            {node: list(neighbors) for node, neighbors in G.items()})

    if len(U) < c:
        partition = set(U)
        U.clear()
        return partition

    partition = set()
    attempts = 0

    try:
        seed = random_seed_selection(G, set(), method="gapless")
        if seed not in U:
            seed = random.choice(list(U))
    except ValueError:
        seed = random.choice(list(U))

    partition.add(seed)
    U.discard(seed)
    # Use a heap-based priority queue. Each element is (priority, node)
    # Priority is defined as -#unassigned_neighbors (so higher connectivity gets higher priority).
    heap = []

    def get_priority(node):
        # Count unassigned neighbors
        return -sum(1 for nbr in G[node] if nbr in U)
    heapq.heappush(heap, (get_priority(seed), seed))

    while heap and len(partition) < c:
        prio, current = heapq.heappop(heap)
        # Expand from current: consider its neighbors that are unassigned and not in precomputed_ap.
        for nbr in G[current]:
            if nbr in U and nbr not in precomputed_ap:
                partition.add(nbr)
                U.discard(nbr)
                heapq.heappush(heap, (get_priority(nbr), nbr))
                if len(partition) >= c:
                    break
        if not heap and len(partition) < c and U:
            # If the heap is empty, pick a new candidate from neighbors of current partition.
            adjacent_candidates = set()
            for node in partition:
                adjacent_candidates |= (G[node] & U)
            new_seed = random.choice(
                list(adjacent_candidates)) if adjacent_candidates else random.choice(list(U))
            partition.add(new_seed)
            U.discard(new_seed)
            heapq.heappush(heap, (get_priority(new_seed), new_seed))
            attempts += 1
            if attempts >= MR:
                logger.warning(
                    f"Partition {p} growth stalled after {MR} retries.")
                break

    return partition


def merge_disconnected_areas(G: Dict, U: Set, Pi: Set) -> Set:
    """
    Merges disconnected subcomponents in Pi using a union–find approach.

    Parameters:
        G: Graph adjacency list.
        U: Unassigned nodes (for interface consistency).
        Pi: The current partition.

    Returns:
        A connected partition (Pi merged).
    """
    # Build the induced subgraph for nodes in Pi.
    induced_adj = {node: {nbr for nbr in G[node] if nbr in Pi} for node in Pi}
    dsu = {node: node for node in Pi}  # Replace class with direct dictionary

    def find(x):
        while x != dsu[x]:
            dsu[x] = dsu[dsu[x]]  # Path compression
            x = dsu[x]
        return x

    def union(x, y):
        dsu[find(y)] = find(x)

    for node, neighbors in induced_adj.items():
        for nbr in neighbors:
            union(node, nbr)

    groups = {}
    for node in Pi:
        rep = find(node)
        groups.setdefault(rep, set()).add(node)

    main_comp = max(groups.values(), key=len)
    main_node = next(iter(main_comp))

    for group in groups.values():
        if group is main_comp:
            continue
        for node in group:
            G[node].add(main_node)
            G[main_node].add(node)

    return Pi


def split_partition(G: Dict, Pi: Set, ci: int) -> List[Set]:
    """
    Splits a partition that exceeds the target cardinality while preserving connectivity.

    Parameters:
        G (Dict): Graph as an adjacency list.
        Pi (Set): The partition to be split.
        ci (int): Target cardinality for each resulting partition.

    Returns:
        List[Set]: List of partitions obtained after splitting.
    """
    if len(Pi) <= ci:
        return [Pi]

    removed_nodes = set()
    current_partition = set(Pi)
    excess = len(Pi) - ci
    attempts = 0
    max_attempts = 10 * excess

    while len(removed_nodes) < excess and attempts < max_attempts:
        mini_adj = {node: list(G[node] & current_partition)
                    for node in current_partition}
        boundary_nodes = find_boundary_areas(current_partition, mini_adj)
        candidates = [
            node for node in boundary_nodes if not is_articulation_point(G, node)]
        if not candidates:
            candidates = list(boundary_nodes)
        if not candidates:
            break
        node_to_remove = random.choice(candidates)
        current_partition.remove(node_to_remove)
        removed_nodes.add(node_to_remove)
        attempts += 1

    removed_adj = {node: [nbr for nbr in G[node]
                          if nbr in removed_nodes] for node in removed_nodes}
    new_components = find_connected_components(removed_adj)

    partitions = [current_partition]
    partitions.extend(new_components)

    logger.info(
        f"Split partition into {len(partitions)} partitions with target cardinality {ci}.")

    return partitions


This is the 'src/utils.py':
"""
utils.py

This module contains utility functions for the P-Regionalization through Recursive Partitioning (PRRP)
algorithm as described in "Statistical Inference for Spatial Regionalization" (SIGSPATIAL 2023)
by Hussah Alrashid, Amr Magdy, and Sergio Rey.
"""

import logging
import random
import concurrent.futures
import heapq
from multiprocessing import Pool, cpu_count
import os
from typing import Dict, List, Set, Any, Tuple, Callable, Iterable
import geopandas as gpd
from shapely.geometry.base import BaseGeometry

# Global flag for parallel processing.
PARALLEL_PROCESSING_ENABLED = False

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
handler.setFormatter(formatter)
if not logger.handlers:
    logger.addHandler(handler)


class DisjointSetUnion:
    """
    A simple Disjoint Set Union (Union-Find) implementation with path compression.
    """

    def __init__(self):
        self.parent = {}

    def find(self, x):
        if self.parent[x] != x:
            self.parent[x] = self.find(self.parent[x])
        return self.parent[x]

    def union(self, x, y):
        rootX = self.find(x)
        rootY = self.find(y)
        if rootX != rootY:
            self.parent[rootY] = rootX


def _has_rook_adjacency(geom1: BaseGeometry, geom2: BaseGeometry) -> bool:
    """
    Checks if two geometries share a rook-adjacent boundary (i.e., a common edge).

    Parameters:
        geom1 (BaseGeometry): The first geometry.
        geom2 (BaseGeometry): The second geometry.

    Returns:
        bool: True if adjacent via a shared edge; False otherwise.
    """
    if not geom1.touches(geom2):
        return False

    intersection = geom1.boundary.intersection(geom2.boundary)
    if intersection.is_empty:
        return False

    if intersection.geom_type in ['LineString', 'MultiLineString']:
        return True

    if intersection.geom_type == 'GeometryCollection':
        for geom in intersection.geoms:
            if geom.geom_type in ['LineString', 'MultiLineString']:
                return True

    return False


def construct_adjacency_list(areas: Any) -> Dict[Any, Set[Any]]:
    """
    Creates a graph adjacency list using rook adjacency for spatial data or by converting a
    pre-constructed graph-based input. For GeoDataFrame inputs, this function uses a ThreadPoolExecutor
    to parallelize processing of each row.

    Parameters:
        areas (GeoDataFrame, list, or dict): Spatial areas with geometry information or a pre-built adjacency list.

    Returns:
        Dict[Any, Set[Any]]: Mapping from area identifiers to sets of adjacent area identifiers.

    Raises:
        TypeError: If the input type is unsupported.
        ValueError: If required geometry information is missing.
    """
    if isinstance(areas, dict):
        for key, value in areas.items():
            if not isinstance(value, set):
                areas[key] = set(value)
        logger.info(
            "Input is a dictionary; converted neighbor lists to sets in-place.")
        return areas

    if isinstance(areas, gpd.GeoDataFrame):
        # Use multi-threading to build the adjacency list.
        adj_list = {}

        def process_row(idx_area):
            idx, area = idx_area
            geom = area.geometry
            neighbors = set()
            possible_matches = list(areas.sindex.intersection(geom.bounds))
            for other_idx in possible_matches:
                if other_idx == idx:
                    continue
                other_geom = areas.loc[other_idx].geometry
                if _has_rook_adjacency(geom, other_geom):
                    neighbors.add(other_idx)
            return idx, neighbors

        with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count()) as executor:
            results = executor.map(process_row, areas.iterrows())
        for idx, nbrs in results:
            adj_list[idx] = nbrs
        logger.info("Adjacency list constructed in parallel from GeoDataFrame.")
        return adj_list

    if isinstance(areas, list):
        # Processing for list of dicts (same as before, sequentially)
        adj_list = {}
        if not all(isinstance(area, dict) for area in areas):
            logger.error(
                "Unsupported list element type; expected dicts with geometry information.")
            raise TypeError(
                "Unsupported type for areas. Expected GeoDataFrame, list of dicts, or dict.")
        if all(area.get('geometry') is None for area in areas):
            for area in areas:
                area_id = area.get('id')
                adj_list[area_id] = {
                    other.get('id') for other in areas if other.get('id') != area_id}
            logger.info(
                "Adjacency list constructed as a complete graph (dummy geometry).")
            return adj_list
        else:
            n = len(areas)
            for i in range(n):
                area_i = areas[i]
                id_i = area_i.get('id', i)
                geom_i = area_i.get('geometry')
                if geom_i is None:
                    logger.error(f"Area with id {id_i} has no geometry.")
                    raise ValueError(f"Area with id {id_i} has no geometry.")
                adj_list[id_i] = set()
                for j in range(n):
                    if i == j:
                        continue
                    area_j = areas[j]
                    id_j = area_j.get('id', j)
                    geom_j = area_j.get('geometry')
                    if geom_j is None:
                        logger.error(f"Area with id {id_j} has no geometry.")
                        raise ValueError(
                            f"Area with id {id_j} has no geometry.")
                    if _has_rook_adjacency(geom_i, geom_j):
                        adj_list[id_i].add(id_j)
            logger.info(
                "Adjacency list constructed from list of dicts based on geometries.")
            return adj_list

    logger.error(
        "Unsupported type for areas. Expected GeoDataFrame, list, or dict.")
    raise TypeError(
        "Unsupported type for areas. Expected GeoDataFrame, list of dicts, or dict.")


def parallel_execute(function: Callable[[Any], Any],
                     data: Iterable[Any],
                     num_threads: int = 1,
                     use_multiprocessing: bool = False) -> List[Any]:
    """
    Executes a function in parallel over the given data.
    Automatically selects between multi-threading and multiprocessing based on the flag.

    Parameters:
        function (Callable[[Any], Any]): The function to apply.
        data (Iterable[Any]): Data items to process.
        num_threads (int): Number of threads/processes to use.
        use_multiprocessing (bool): Whether to use multiprocessing.

    Returns:
        List[Any]: List of results.
    """
    if num_threads > 1 and PARALLEL_PROCESSING_ENABLED:
        if use_multiprocessing:
            logger.info("Using multiprocessing for parallel execution.")
            with Pool(min(num_threads, cpu_count())) as pool:
                results = pool.map(function, list(data))
        else:
            logger.info("Using threading for parallel execution.")
            with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
                results = list(executor.map(function, data))
        logger.info("Parallel execution completed successfully.")
        return results
    else:
        logger.info(
            "Parallel processing disabled or num_threads <= 1. Executing sequentially.")
        results = [function(item) for item in data]
        logger.info("Sequential execution completed successfully.")
        return results


def find_articulation_points(G: Dict[int, List[int]]) -> Set[int]:
    """
    Computes the articulation points of a graph using Tarjan’s Algorithm in O(V + E) time.

    Parameters:
        G (Dict[int, List[int]]): Graph represented as an adjacency list.

    Returns:
        Set[int]: The set of articulation points.
    """
    adj = {u: set(neighbors) for u, neighbors in G.items()}
    if not adj:
        return set()

    disc: Dict[int, int] = {}
    low: Dict[int, int] = {}
    parent: Dict[int, int] = {}
    ap: Set[int] = set()
    time_counter = 0
    child_count: Dict[int, int] = {}
    stack_frame = []

    for u in adj:
        if u in disc:
            continue
        parent[u] = None
        disc[u] = time_counter
        low[u] = time_counter
        time_counter += 1
        child_count[u] = 0
        stack_frame.append((u, iter(adj[u])))

        while stack_frame:
            current, neighbors_iter = stack_frame[-1]
            try:
                v = next(neighbors_iter)
                if v not in disc:
                    parent[v] = current
                    child_count[current] = child_count.get(current, 0) + 1
                    disc[v] = time_counter
                    low[v] = time_counter
                    time_counter += 1
                    child_count[v] = 0
                    stack_frame.append((v, iter(adj[v])))
                elif v != parent[current]:
                    low[current] = min(low[current], disc[v])
            except StopIteration:
                stack_frame.pop()
                if stack_frame:
                    par, _ = stack_frame[-1]
                    low[par] = min(low[par], low[current])
                    if parent[current] == par and parent[par] is not None and low[current] >= disc[par]:
                        ap.add(par)
                else:
                    if child_count[current] > 1:
                        ap.add(current)
    return ap


def find_connected_components(adj_list: Dict[Any, List[Any]]) -> List[Set[Any]]:
    """
    Finds all connected components in a graph using DFS.

    Parameters:
        adj_list (Dict[Any, List[Any]]): The graph's adjacency list.

    Returns:
        List[Set[Any]]: A list of connected components (each component is a set of nodes).
    """
    visited: Set[Any] = set()
    components: List[Set[Any]] = []

    for node in adj_list.keys():
        if node not in visited:
            component: Set[Any] = set()
            stack: List[Any] = [node]
            while stack:
                n = stack.pop()
                if n not in visited:
                    visited.add(n)
                    component.add(n)
                    for neighbor in adj_list.get(n, []):
                        if neighbor not in visited:
                            stack.append(neighbor)
            components.append(component)

    logger.info(f"Found {len(components)} connected component(s).")
    return components


def is_articulation_point(adj_list: Dict[Any, List[Any]], node: Any) -> bool:
    """
    Determines whether a node is an articulation point using Tarjan's Algorithm.

    A node is an articulation point if its removal increases the number of connected components.

    Parameters:
        adj_list (Dict[Any, List[Any]]): The graph's adjacency list.
        node (Any): The node to check.

    Returns:
        bool: True if the node is an articulation point; False otherwise.
    """
    if node not in adj_list:
        logger.error(f"Node {node} not found in the adjacency list.")
        raise KeyError(f"Node {node} not found in the adjacency list.")

    # ✅ **Fix: Leaf nodes are never articulation points**
    if len(adj_list[node]) <= 1:
        logger.info(
            f"Node {node} is a leaf node and cannot be an articulation point.")
        return False

    def tarjan_ap_util(v: Any, parent: Any, disc: Dict[Any, int],
                       low: Dict[Any, int], time: List[int], ap: Set[Any]) -> None:
        children = 0
        disc[v] = low[v] = time[0]
        time[0] += 1

        for w in adj_list[v]:
            if w not in disc:
                children += 1
                tarjan_ap_util(w, v, disc, low, time, ap)
                low[v] = min(low[v], low[w])
                if parent is None and children > 1:
                    ap.add(v)
                if parent is not None and low[w] >= disc[v]:
                    ap.add(v)
            elif w != parent:
                low[v] = min(low[v], disc[w])

    disc: Dict[Any, int] = {}
    low: Dict[Any, int] = {}
    time: List[int] = [0]
    ap: Set[Any] = set()

    for v in adj_list.keys():
        if v not in disc:
            tarjan_ap_util(v, None, disc, low, time, ap)

    is_ap = node in ap
    logger.info(
        f"Node {node} is {'an' if is_ap else 'not an'} articulation point.")

    return is_ap


def remove_articulation_area(adj_list: Dict[Any, List[Any]], node: Any) -> Dict[Any, List[Any]]:
    """
    Removes an articulation point from the graph and reassigns it to maintain connectivity.
    If removal disconnects the graph, reassigns the node to the largest connected component.

    Parameters:
        adj_list (Dict[Any, List[Any]]): The graph's adjacency list.
        node (Any): The articulation point to remove.

    Returns:
        Dict[Any, List[Any]]: The updated adjacency list.
    """
    if node not in adj_list:
        logger.error(f"Node {node} not found in the adjacency list.")
        raise KeyError(f"Node {node} not in adjacency list.")

    new_adj = {k: list(v) for k, v in adj_list.items()}
    original_neighbors = new_adj[node]

    for neighbor in original_neighbors:
        if node in new_adj[neighbor]:
            new_adj[neighbor].remove(node)
    del new_adj[node]

    components = find_connected_components(new_adj)
    if len(components) > 1:
        largest_component = max(components, key=len)
        new_adj[node] = []
        for neighbor in original_neighbors:
            if neighbor in largest_component:
                new_adj[node].append(neighbor)
                new_adj[neighbor].append(node)
        logger.warning(
            f"Articulation node {node} removed and reassigned to maintain connectivity.")
    else:
        new_adj[node] = [original_neighbors[0]]
        new_adj[original_neighbors[0]].append(node)
        logger.info(
            f"Node {node} was removed but reassigned to maintain connectivity.")

    return new_adj


def random_seed_selection(adj_list: Dict[Any, List[Any]], assigned_regions: Set[Any], method: str = "gapless") -> Any:
    """
    Selects a seed node efficiently from unassigned nodes using a heap-based approach.
    This function computes connectivity scores for unassigned nodes once and uses a min-heap to retrieve
    the top candidates quickly. The score is defined as the number of neighbors in assigned_regions.

    Parameters:
        adj_list (Dict[Any, List[Any]]): The graph's adjacency list.
        assigned_regions (Set[Any]): Nodes that are already assigned.
        method (str): Selection method. Currently, only "gapless" is supported.

    Returns:
        Any: A selected seed node.

    Raises:
        ValueError: If no unassigned nodes are available or if the method is unknown.
    """
    unassigned = set(adj_list.keys()) - assigned_regions
    if not unassigned:
        logger.error("No unassigned nodes available for seed selection.")
        raise ValueError("No unassigned nodes available.")

    if method == "gapless":
        # Build a heap of (negative_score, node) for the top candidates.
        # The score is the number of neighbors in assigned_regions.
        heap = []
        for node in unassigned:
            score = sum(1 for nbr in adj_list[node] if nbr in assigned_regions)
            # We want high scores to come first, so use negative score.
            heapq.heappush(heap, (-score, node))
        # Retrieve the top candidate from the heap (if there are several with equal score, one is chosen randomly).
        # For extra randomness, we extract up to 10 best candidates and pick one.
        top_candidates = []
        for _ in range(min(10, len(heap))):
            top_candidates.append(heapq.heappop(heap)[1])
        chosen = random.choice(top_candidates)
        logger.info(
            f"Selected gapless seed: {chosen} from top candidates {top_candidates}")

        return chosen
    else:
        logger.error(f"Unknown seed selection method: {method}")
        raise ValueError(f"Unknown seed selection method: {method}")


def load_graph_from_metis(file_path: str) -> Dict[int, List[int]]:
    """
    Reads a graph in METIS format and converts it into an adjacency list.

    Parameters:
        file_path (str): Path to the METIS format file.

    Returns:
        Dict[int, List[int]]: Adjacency list with 1-based node indices.

    Raises:
        ValueError: If the file is empty, header is invalid, or malformed.
    """
    adj_list: Dict[int, List[int]] = {}
    try:
        with open(file_path, 'r') as f:
            lines = f.readlines()

        if not lines:
            logger.error("METIS file is empty.")
            raise ValueError("Empty METIS file.")

        content_lines = [line.strip() for line in lines if line.strip()
                         and not line.strip().startswith('%')]
        if not content_lines:
            logger.error(
                "METIS file is empty or contains only comments/whitespace.")
            raise ValueError("Empty METIS file.")

        header_tokens = content_lines[0].split()
        if len(header_tokens) < 2:
            logger.error(
                "Invalid METIS header: requires at least two tokens (num_nodes and num_edges).")
            raise ValueError("Invalid METIS header: not enough tokens.")
        num_nodes = int(header_tokens[0])
        header_edge_count = int(header_tokens[1])

        if len(content_lines) - 1 < num_nodes:
            logger.error("Insufficient vertex lines in METIS file.")
            raise ValueError("Insufficient vertex lines in METIS file.")

        for i in range(num_nodes):
            line = content_lines[i + 1]
            tokens = line.split()
            neighbors = []
            for token in tokens:
                try:
                    neighbor = int(token)
                except Exception as e:
                    logger.error(
                        f"Vertex {i+1}: Invalid neighbor token '{token}'.")
                    raise ValueError(
                        f"Invalid neighbor token in vertex {i+1}.") from e
                if neighbor != i + 1:
                    neighbors.append(neighbor)
            for neighbor in neighbors:
                if neighbor < 1 or neighbor > num_nodes:
                    logger.error(
                        f"Vertex {i+1}: Neighbor index {neighbor} out of range (1 to {num_nodes}).")
                    raise ValueError(
                        f"Neighbor index out of range in vertex {i+1}.")
            adj_list[i + 1] = neighbors

        total_neighbor_entries = sum(len(nlist) for nlist in adj_list.values())
        if total_neighbor_entries == header_edge_count:
            final_num_edges = header_edge_count
        elif total_neighbor_entries == 2 * header_edge_count:
            final_num_edges = header_edge_count
        else:
            final_num_edges = total_neighbor_entries // 2
            logger.warning(
                f"Computed total neighbor entries ({total_neighbor_entries}) do not match header edge count ({header_edge_count}). Using computed edge count: {final_num_edges}.")

        logger.info(
            f"Loaded METIS graph from '{file_path}': {num_nodes} nodes, {final_num_edges} edges.")
        return adj_list

    except Exception as e:
        logger.error(f"Failed to load METIS graph from {file_path}: {e}")
        raise


def save_graph_to_metis(file_path: str, adj_list: Dict[int, List[int]]) -> None:
    """
    Saves an adjacency list in METIS format.

    Parameters:
        file_path (str): File path for the METIS graph.
        adj_list (Dict[int, List[int]]): The adjacency list.

    Returns:
        None
    """
    try:
        num_nodes = len(adj_list)
        num_edges = sum(len(neighbors) for neighbors in adj_list.values()) // 2
        with open(file_path, 'w') as f:
            f.write(f"{num_nodes} {num_edges}\n")
            for i in range(1, num_nodes + 1):
                neighbors = adj_list.get(i, [])
                f.write(" ".join(map(str, neighbors)) + "\n")
        logger.info(
            f"Graph saved to {file_path} in METIS format successfully.")
    except Exception as e:
        logger.error(
            f"Failed to save graph to METIS format at {file_path}: {e}")
        raise


def find_boundary_areas(region: Set[Any], adj_list: Dict[Any, List[Any]]) -> Set[Any]:
    """
    Identifies boundary areas of a region.

    Parameters:
        region (Set[Any]): Nodes in the region.
        adj_list (Dict[Any, List[Any]]): Graph's adjacency list.

    Returns:
        Set[Any]: Boundary nodes with at least one neighbor outside the region.
    """
    boundary: Set[Any] = set()
    for node in region:
        for neighbor in adj_list.get(node, []):
            if neighbor not in region:
                boundary.add(node)
                break
    logger.info(f"Identified {len(boundary)} boundary area(s) in the region.")
    return boundary


def calculate_low_link_values(adj_list: Dict[Any, List[Any]]) -> Tuple[Dict[Any, int], Dict[Any, int]]:
    """
    Computes discovery and low-link values for nodes in the graph using DFS.

    Parameters:
        adj_list (Dict[Any, List[Any]]): Graph's adjacency list.

    Returns:
        Tuple[Dict[Any, int], Dict[Any, int]]: (disc, low) mappings.
    """
    disc: Dict[Any, int] = {}
    low: Dict[Any, int] = {}
    time: List[int] = [0]

    def dfs(u: Any, parent: Any) -> None:
        disc[u] = low[u] = time[0]
        time[0] += 1
        for v in adj_list[u]:
            if v not in disc:
                dfs(v, u)
                low[u] = min(low[u], low[v])
            elif v != parent:
                low[u] = min(low[u], disc[v])

    for node in adj_list.keys():
        if node not in disc:
            dfs(node, None)

    logger.info("Calculated low-link values for all nodes.")
    return disc, low


def compute_degree_list(G: Dict[int, List[int]]) -> Dict[int, List[int]]:
    """
    Computes the degree list for each node and establishes parent-child relationships.

    Parameters:
        G (Dict[int, List[int]]): Graph adjacency list.

    Returns:
        Dict[int, List[int]]: Mapping of each node to its child nodes.
    """
    degrees = {node: len(neighbors) for node, neighbors in G.items()}
    is_parent = {}
    for node, neighbors in G.items():
        if not neighbors:
            is_parent[node] = False
            continue
        neighbor_degrees = [degrees[nb] for nb in neighbors if nb in degrees]
        sorted_degs = sorted(neighbor_degrees)
        n = len(sorted_degs)
        median = sorted_degs[n // 2] if n % 2 == 1 else (
            sorted_degs[n // 2 - 1] + sorted_degs[n // 2]) / 2
        is_parent[node] = degrees[node] > median

    degree_list = {node: [] for node in G}
    processed_edges = set()
    for u in G:
        for v in G[u]:
            if v not in G:
                continue
            if u not in G[v]:
                continue
            edge = tuple(sorted((u, v)))
            if edge in processed_edges:
                continue
            processed_edges.add(edge)
            if is_parent.get(u, False) and (not is_parent.get(v, False)) and (degrees[u] > degrees[v]):
                degree_list[u].append(v)
            elif is_parent.get(v, False) and (not is_parent.get(u, False)) and (degrees[v] > degrees[u]):
                degree_list[v].append(u)

    return degree_list


def parallel_execute(function: Callable[[Any], Any],
                     data: Iterable[Any],
                     num_threads: int = 1,
                     use_multiprocessing: bool = False) -> List[Any]:
    """
    Executes a function in parallel over the given data if enabled.

    Parameters:
        function (Callable[[Any], Any]): The function to apply.
        data (Iterable[Any]): Data items to process.
        num_threads (int): Number of threads/processes to use.
        use_multiprocessing (bool): Whether to use multiprocessing.

    Returns:
        List[Any]: List of results.
    """
    if num_threads > 1 and PARALLEL_PROCESSING_ENABLED:
        if use_multiprocessing:
            logger.info("Using multiprocessing for parallel execution.")
            with Pool(min(num_threads, cpu_count())) as pool:
                results = pool.map(function, list(data))
        else:
            logger.info("Using threading for parallel execution.")
            with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
                results = list(executor.map(function, data))
        logger.info("Parallel execution completed successfully.")
        return results
    else:
        logger.info(
            "Parallel processing disabled or num_threads <= 1. Executing sequentially.")
        results = [function(item) for item in data]
        logger.info("Sequential execution completed successfully.")
        return results


This is the 'src/pymetis_partition.py':
import metis
import logging
from typing import Dict, List, Set

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)


def partition_graph_pymetis(adj_list: Dict[int, List[int]], num_partitions: int) -> Dict[int, Set[int]]:
    """
    Partitions a graph using PyMETIS into the specified number of partitions.

    Parameters:
        adj_list (Dict[int, List[int]]): The adjacency list of the graph.
        num_partitions (int): The number of partitions to divide the graph into.

    Returns:
        Dict[int, Set[int]]: A dictionary mapping partition indices to sets of node IDs.
    """
    if num_partitions <= 1:
        logger.error("Number of partitions must be greater than 1.")
        raise ValueError("Number of partitions must be greater than 1.")

    # Ensure nodes are sequential integers starting from 0 (required by METIS)
    node_map = {node: idx for idx, node in enumerate(sorted(adj_list.keys()))}
    reverse_map = {idx: node for node, idx in node_map.items()}

    # Convert adjacency list to METIS format (list of lists)
    metis_graph = [[node_map[neighbor] for neighbor in adj_list[node]]
                   for node in sorted(adj_list.keys())]

    try:
        logger.info(
            f"Partitioning graph using PyMETIS into {num_partitions} partitions...")
        _, partitions = metis.part_graph(metis_graph, nparts=num_partitions)
    except Exception as e:
        logger.error(f"PyMETIS partitioning failed: {e}")
        raise RuntimeError(f"PyMETIS failed due to: {e}")

    # Organize partitions into dictionary format
    partition_dict = {i: set() for i in range(num_partitions)}
    for idx, part_id in enumerate(partitions):
        original_node = reverse_map[idx]  # Map back to original node IDs
        partition_dict[part_id].add(original_node)

    logger.info("PyMETIS partitioning completed successfully.")
    return partition_dict


This is the 'src/prrp_data_loader.py':
import os
import networkx as nx
import geopandas as gpd
import logging
import numpy as np

# Configure logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
if not logger.handlers:
    logger.addHandler(handler)

def load_shapefile(file_path: str) -> list:
    """
    Loads a shapefile into a list of mappings between area IDs and their gemetry.

    Parameters:
        file_path: Path to the shapefile
    
    Returns:
        list: List of mappings between area IDs and their geometry
    """
    try:
        gdf = gpd.read_file(file_path)
    except FileNotFoundError:
        logger.error(f"File not found: {file_path}")
        return None
    except Exception as e:
        logger.error(f"Error reading file {file_path}: {e}")
        return None
    
    try:
        area_data = []
        for idx, row in gdf.iterrows():
            area_data.append({"id": row["GEOID"], "geometry": row["geometry"]})
    except Exception as e:
        logger.error(f"Error parsing shapefile: {e}")
        return None
    
    return area_data

def load_metis_graph(file_path: str) -> (nx.Graph, dict):
    """
    Parses a METIS graph file and loads it into an adjacency list.

    Parameters:
        file_path: Path to the METIS graph file

    Returns:
        nx.Graph: NetworkX graph object
        dict: Adjacency list
    """
    try:
        with open(file_path, "r") as f:
            lines = f.readlines()
    except FileNotFoundError:
        logger.error(f"File not found: {file_path}")
        return None, None
    except Exception as e:
        logger.error(f"Error reading file {file_path}: {e}")
        return None, None

    try:
        header = lines[0].strip().split()
        num_nodes, num_edges = int(header[0]), int(header[1])
    except Exception as e:
        logger.error(f"Error parsing header in file {file_path}: {e}")
        return None, None

    adjacency_list = {}

    try:
        for node_id, line in enumerate(lines[1:], start=1):
            neighbors = list(map(lambda x: int(x) - 1, line.strip().split()))
            adjacency_list[node_id - 1] = neighbors  # Convert to 0-based index
    except Exception as e:
        logger.error(f"Error parsing adjacency list in file {file_path}: {e}")
        return None, None

    G = nx.Graph()
    try:
        for node, neighbors in adjacency_list.items():
            for neighbor in neighbors:
                G.add_edge(node, neighbor)
    except Exception as e:
        logger.error(f"Error creating NetworkX graph: {e}")
        return None, None

    return G, adjacency_list

def preprocess_graph(G: nx.Graph, remove_self_loops=True) -> nx.Graph:
    """
    Preprocesses the graph by removing self-loops and ensuring it is undirected.

    Parameters:
        G: NetworkX graph object
        remove_self_loops: Boolean flag to remove self-loops

    Returns:
        nx.Graph: Preprocessed graph
    """
    try:
        if remove_self_loops:
            G.remove_edges_from(nx.selfloop_edges(G))
            logger.debug("Removed self-loops.")

        G = G.to_undirected()
        logger.debug("Converted to undirected graph.")
    except Exception as e:
        logger.error(f"Error preprocessing graph: {e}")
        return None

    return G

def convert_to_adjacency_list(G: nx.Graph) -> dict:
    """
    Converts the graph to an adjacency list format.

    Parameters:
        G: NetworkX graph object

    Returns:
        dict: Dictionary representing adjacency list
    """
    try:
        return {node: list(G.neighbors(node)) for node in G.nodes()}
    except Exception as e:
        logger.error(f"Error converting to adjacency list: {e}")
        return None

def convert_to_edge_list(G: nx.Graph) -> list:
    """
    Converts the graph to an edge list format.

    Parameters:
        G: NetworkX graph object

    Returns:
        list: List of edges
    """
    try:
        return list(G.edges())
    except Exception as e:
        logger.error(f"Error converting to edge list: {e}")
        return None

def convert_to_adjacency_matrix(G: nx.Graph) -> np.ndarray:
    """
    Converts the graph to an adjacency matrix.

    Parameters:
        G: NetworkX graph object

    Returns:
        np.ndarray: NumPy adjacency matrix
    """
    try:
        return nx.to_numpy_array(G)
    except Exception as e:
        logger.error(f"Error converting to adjacency matrix: {e}")
        return None

